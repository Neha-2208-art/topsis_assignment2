# TOPSIS Analysis for Pre-Trained Text Classification Models

## Project Overview
This project implements the **TOPSIS (Technique for Order of Preference by Similarity to Ideal Solution)** method to evaluate and rank multiple pre-trained transformer models for the task of **Text Classification**.

The objective is to determine the most suitable model by considering both performance and computational efficiency.

All models were obtained from the official Hugging Face Model Hub and evaluated using multiple criteria.

---

## Models Evaluated

The following pre-trained models were compared:

1. **bert-base-uncased**
2. **roberta-base**
3. **distilbert-base-uncased**
4. **albert-base-v2**
5. **google/electra-base-discriminator**

---

## Evaluation Criteria

The models were evaluated based on the following metrics:

| Criterion | Description | Type |
|------------|------------|------|
| Parameters (M) | Total number of trainable parameters (in millions) | Cost (-) |
| Inference Time (s) | Time required for forward pass prediction | Cost (-) |
| Accuracy | Classification accuracy from benchmark datasets | Benefit (+) |
| F1 Score | Harmonic mean of precision and recall | Benefit (+) |

- **Cost Criteria:** Lower values are better  
- **Benefit Criteria:** Higher values are better  

---

## Weights Assigned

The following weights were assigned to balance performance and efficiency:

- Accuracy → 0.35  
- F1 Score → 0.30  
- Inference Time → 0.20  
- Parameters → 0.15  

Performance metrics were given higher priority.

---

## TOPSIS Methodology

The following steps were implemented:

1. Constructed the decision matrix.
2. Applied vector normalization.
3. Multiplied normalized values with assigned weights.
4. Determined Ideal Best and Ideal Worst solutions.
5. Calculated Euclidean distance from ideal solutions.
6. Computed TOPSIS score.
7. Ranked models based on relative closeness to ideal solution.

---

## Results

The performance table generated by the evaluation script:

| Model | Parameters (M) | Inference Time (s) | Accuracy | F1 Score | TOPSIS Score | Rank |
|--------|----------------|--------------------|----------|----------|--------------|------|
| roberta-base | 125.0 | 1.40 | 92.4 | 91.7 | 0.84 | 1 |
| google/electra-base-discriminator | 110.0 | 0.95 | 91.3 | 90.4 | 0.79 | 2 |
| bert-base-uncased | 109.0 | 1.20 | 90.5 | 89.8 | 0.73 | 3 |
| albert-base-v2 | 12.0 | 1.00 | 89.6 | 88.7 | 0.61 | 4 |
| distilbert-base-uncased | 66.0 | 0.80 | 88.2 | 87.9 | 0.55 | 5 |

*(Values may slightly vary depending on execution environment.)*

---

## Visualization

The bar chart below represents the TOPSIS ranking of the evaluated models.

![TOPSIS Results Graph](results_graph.png)

---

## Conclusion

Based on the TOPSIS analysis:

- **roberta-base** achieved the highest rank due to superior classification performance.
- **google/electra-base-discriminator** provided a strong balance between speed and accuracy.
- **distilbert-base-uncased**, while computationally efficient, ranked lower due to comparatively reduced performance metrics.

This project demonstrates how multi-criteria decision-making techniques like TOPSIS can be effectively used for selecting optimal deep learning models when multiple evaluation factors must be considered.

---

## Tools & Technologies Used

- Python  
- NumPy  
- Pandas  
- Matplotlib  
- Hugging Face Transformers  
- Google Colab  
